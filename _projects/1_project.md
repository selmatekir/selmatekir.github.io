---
layout: page
title: Viral Evrim ve Kaçış Tespiti için Dil Modellemesi
description: with background image
img: assets/img/workflow_escape_2.jpg
importance: 1
category: work
related_publications: true
---

Every project has a beautiful feature showcase page.
It's easy to include images in a flexible 3-column grid format.
Make your photos 1/3, 2/3, or full width.

To give your project a background in the portfolio page, just add the img tag to the front matter like so:

    ---
    layout: page
    title: project
    description: a project with a background image
    img: /assets/img/12.jpg
    ---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/1.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/3.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/5.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Caption photos easily. On the left, a road goes through a tunnel. Middle, leaves artistically fall in a hipster photoshoot. Right, in another hipster photoshoot, a lumberjack grasps a handful of pine needles.
</div>

Impact of Contrastive Learning

We analyze the distribution of CSCS rankings over 6000 mutated sequences, including Eris, New Omicron, and those generated by ProtGPT2 as a control condition. We run the viral escape prediction workflow in Section \ref{sec:escape} and calculate semantic change (SC), sequence probability (SP), and inverse perplexity (IP) values. The average embedding of Omicron sequences are used as a reference to calculate SC, SP, and IP for each sequence. We compare results of three different models: (1) Pre-trained CoV-RoBERTa model before CoV-SNN training, (2) CoV-RoBERTa model extracted from CoV-SNN model with best test accuracy (Checkpoint 1 with 100% test accuracy), and (3) CoV-RoBERTa model extracted from CoV-SNN model with the best zero-shot accuracy (Checkpoint 5 with 91.5% zero-shot accuracy).

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/cscs_scatter.jpg" title="Distribution of semantic change versus sequence probability and inverse perplexity. " class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Top: Pre-trained CoV-RoBERTa model before CoV-SNN training. Middle: CoV-RoBERTa model extracted from CoV-SNN model with best test accuracy (Checkpoint 1). Bottom: CoV-RoBERTa model extracted from CoV-SNN model with best zero-shot accuracy (Checkpoint 5).
</div>

Figure presents the distribution of semantic change, sequence probability, and inverse perplexity values for Eris, ProtGPT2, and New Omicron sequences. The model without contrastive learning (at top of the Figure) is not able to differentiate Eris sequences from New Omicron or ProtGPT2 sequences, as expected. The clustering is better with the model with best test accuracy (in the middle) which can show Eris sequences introduce higher semantic change (thus, higher escape potential) compared to others. On the other hand, the best results are achieved with a clear separation with the model with best zero-shot accuracy (at the bottom). In all experiments, ProtGPT2 sequences show a similar semantic change and grammaticality distribution as the New Omicron sequences, indicating that the model effectively produces meaningful sequences. For the same reason, most ProtGPT2 points (yellow) are positioned under New Omicron points (red) in the bottom plot.

You can also put regular text between your rows of images, even citations {% cite einstein1950meaning %}.
Say you wanted to write a bit about your project before you posted the rest of the images.
You describe how you toiled, sweated, _bled_ for your project, and then... you reveal its glory in the next row of images.

<div class="row justify-content-sm-center">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/6.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm-4 mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/11.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    You can also have artistically styled 2/3 + 1/3 images, like these.
</div>

The code is simple.
Just wrap your images with `<div class="col-sm">` and place them inside `<div class="row">` (read more about the <a href="https://getbootstrap.com/docs/4.4/layout/grid/">Bootstrap Grid</a> system).
To make images responsive, add `img-fluid` class to each; for rounded corners and shadows use `rounded` and `z-depth-1` classes.
Here's the code for the last row of images above:

{% raw %}

```html
<div class="row justify-content-sm-center">
  <div class="col-sm-8 mt-3 mt-md-0">
    {% include figure.liquid path="assets/img/6.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
  </div>
  <div class="col-sm-4 mt-3 mt-md-0">
    {% include figure.liquid path="assets/img/11.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
  </div>
</div>
```

{% endraw %}
